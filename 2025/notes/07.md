# Day 7 - Laboratories

> [<- Yesterday](06.md) | [Tomorrow ->](08.md)

A great puzzle, with a fun twist. Keep thinking I did it fast but then I look at Discord and [TJ O](https://github.com/TJThePiGuy)'s already finished like 12 minutes ago...

|      | Part One | Part Two | Total |
|------|----------|----------|-------|
| Time | 7:43     | 11:45    | 19:28 |

## Part One
Using my helper, I imported as a grid and then to avoid counting splits twice (since beams can merge), I used a set to keep track of the next bunch of beams. Repeat until bottom.

[Code](../code/07a.py)

```python
from helpers.datagetter import aocd_data_in
from helpers.matrix import from_grid

din, aocd_submit = aocd_data_in(split=False, numbers=False)

grid = from_grid(din)

ans = 0
beams = list(grid.findall('S'))

y = beams[0][0]
while y < grid.size[0]:
    new_beams = set()
    for beam in beams:
        if grid.get((beam[0] + 1, beam[1])) == "^":
            ans += 1
            if grid._is_in_bounds((beam[0] + 1, beam[1] - 1)):
                new_beams.add((beam[0] + 1, beam[1] - 1))
            if grid._is_in_bounds((beam[0] + 1, beam[1] + 1)):
                new_beams.add((beam[0] + 1, beam[1] + 1))
        else:
            new_beams.add((beam[0] + 1, beam[1]))
    y += 1
    beams = list(new_beams)

aocd_submit(ans)
```

## Part Two
Cool twist! Count the number of unique paths without counting each and every one. We forget about beam merging, since unique paths all count. So I used dynamic programming to avoid recalculating and storing the position of beams that share the same space in multiple universes. All we need to know is *how many unique beams are in any given location*, and then add the effect of all of them moving or splitting to the next positions down. The sum of the beams at each location at the bottom is the final answer for number of universes, as each position will contain the number of paths you could take to reach that position.

I split the beam generation code to a function because I was originally thinking that was the part to DP (so I was going to use `@functools.cache`), but I quickly realized it was just a memory issue storing all the beams, and it was slowing down around half-way down my input. So instead I used dynamic programming for efficiently storing trillions of beam positions in a dictionary!

[Code](../code/07b.py)

```python
from helpers.datagetter import aocd_data_in
from helpers.matrix import from_grid
from collections import defaultdict

din, aocd_submit = aocd_data_in(split=False, numbers=False)

grid = from_grid(din)

ans = 0
beams = defaultdict(int)
beams[list(grid.findall('S'))[0]] = 1


def get_next_beams(beam):
    new_beams = []
    if grid.get((beam[0] + 1, beam[1])) == "^":
        if grid._is_in_bounds((beam[0] + 1, beam[1] - 1)):
            new_beams.append((beam[0] + 1, beam[1] - 1))
        if grid._is_in_bounds((beam[0] + 1, beam[1] + 1)):
            new_beams.append((beam[0] + 1, beam[1] + 1))
    else:
        new_beams.append((beam[0] + 1, beam[1]))
    return new_beams


y = list(beams.keys())[0][0]
while y < grid.size[0]:
    new_beams = defaultdict(int)
    for beam in beams:
        for nbeam in get_next_beams(beam):
            new_beams[nbeam] += beams[beam]

    beams = new_beams
    y += 1

aocd_submit(sum(beams.values()))
```